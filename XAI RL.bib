
@article{lesort_state_2018,
	title = {State {Representation} {Learning} for {Control}: {An} {Overview}},
	volume = {108},
	issn = {08936080},
	shorttitle = {State {Representation} {Learning} for {Control}},
	url = {http://arxiv.org/abs/1802.04181},
	doi = {10.1016/j.neunet.2018.07.006},
	abstract = {Representation learning algorithms are designed to learn abstract features that characterize data. State representation learning (SRL) focuses on a particular kind of representation learning where learned features are in low dimension, evolve through time, and are influenced by actions of an agent. The representation is learned to capture the variation in the environment generated by the agent's actions; this kind of representation is particularly suitable for robotics and control scenarios. In particular, the low dimension characteristic of the representation helps to overcome the curse of dimensionality, provides easier interpretation and utilization by humans and can help improve performance and speed in policy learning algorithms such as reinforcement learning. This survey aims at covering the state-of-the-art on state representation learning in the most recent years. It reviews different SRL methods that involve interaction with the environment, their implementations and their applications in robotics control tasks (simulated or real). In particular, it highlights how generic learning objectives are differently exploited in the reviewed algorithms. Finally, it discusses evaluation methods to assess the representation learned and summarizes current and future lines of research.},
	urldate = {2025-03-10},
	journal = {Neural Networks},
	author = {Lesort, Timothée and Díaz-Rodríguez, Natalia and Goudou, Jean-François and Filliat, David},
	month = dec,
	year = {2018},
	note = {arXiv:1802.04181 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	pages = {379--392},
	file = {Full Text PDF:C\:\\Users\\jackb\\Zotero\\storage\\7EMI8XQW\\Lesort et al. - 2018 - State Representation Learning for Control An Overview.pdf:application/pdf;Snapshot:C\:\\Users\\jackb\\Zotero\\storage\\IP588IUZ\\1802.html:text/html},
}

@misc{kalra_rlinspect_2024,
	title = {{RLInspect}: {An} {Interactive} {Visual} {Approach} to {Assess} {Reinforcement} {Learning} {Algorithm}},
	shorttitle = {{RLInspect}},
	url = {http://arxiv.org/abs/2411.08392},
	doi = {10.48550/arXiv.2411.08392},
	abstract = {Reinforcement Learning (RL) is a rapidly growing area of machine learning that finds its application in a broad range of domains, from finance and healthcare to robotics and gaming. Compared to other machine learning techniques, RL agents learn from their own experiences using trial and error, and improve their performance over time. However, assessing RL models can be challenging, which makes it difficult to interpret their behaviour. While reward is a widely used metric to evaluate RL models, it may not always provide an accurate measure of training performance. In some cases, the reward may seem increasing while the model's performance is actually decreasing, leading to misleading conclusions about the effectiveness of the training. To overcome this limitation, we have developed RLInspect - an interactive visual analytic tool, that takes into account different components of the RL model - state, action, agent architecture and reward, and provides a more comprehensive view of the RL training. By using RLInspect, users can gain insights into the model's behaviour, identify issues during training, and potentially correct them effectively, leading to a more robust and reliable RL system.},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Kalra, Geetansh and Singh, Divye and Jose, Justin},
	month = nov,
	year = {2024},
	note = {arXiv:2411.08392 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Full Text PDF:C\:\\Users\\jackb\\Zotero\\storage\\CK5F86M3\\Kalra et al. - 2024 - RLInspect An Interactive Visual Approach to Assess Reinforcement Learning Algorithm.pdf:application/pdf;Snapshot:C\:\\Users\\jackb\\Zotero\\storage\\RQWGS729\\2411.html:text/html},
}

@misc{gebauer_pitfall_2021,
	title = {The {Pitfall} of {More} {Powerful} {Autoencoders} in {Lidar}-{Based} {Navigation}},
	url = {http://arxiv.org/abs/2102.02127},
	doi = {10.48550/arXiv.2102.02127},
	abstract = {The benefit of pretrained autoencoders for reinforcement learning in comparison to training on raw observations is already known [1]. In this paper, we address the generation of a compact and information-rich state representation. In particular, we train a variational autoencoder for 2D-lidar scans to use its latent state for reinforcement learning of navigation tasks. To achieve high reconstruction power of our autoencoding pipeline, we propose an - in the context of autoencoding 2D-lidar scans - novel preprocessing into a local binary occupancy image. This has no additional requirements, neither self-localization nor robust mapping, and therefore can be applied in any setting and easily transferred from simulation in real-world. In a second stage, we show the usage of the compact state representation generated by our autoencoding pipeline in a simplistic navigation task and expose the pitfall that increased reconstruction power will always lead to an improved performance. We implemented our approach in python using tensorflow. Our datasets are simulated with pybullet as well as recorded using a slamtec rplidar A3. The experiments show the significantly improved reconstruction capabilities of our approach for 2D-lidar scans w.r.t. the state of the art. However, as we demonstrate in the experiments the impact on reinforcement learning in lidar-based navigation tasks is non-predictable when improving the latent state representation generated by an autoencoding pipeline. This is surprising and needs to be taken into account during the process of optimizing a pretrained autoencoder for reinforcement learning tasks.},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Gebauer, Christopher and Bennewitz, Maren},
	month = mar,
	year = {2021},
	note = {arXiv:2102.02127 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Full Text PDF:C\:\\Users\\jackb\\Zotero\\storage\\6WMMUGEL\\Gebauer and Bennewitz - 2021 - The Pitfall of More Powerful Autoencoders in Lidar-Based Navigation.pdf:application/pdf;Snapshot:C\:\\Users\\jackb\\Zotero\\storage\\ICMGPPPD\\2102.html:text/html},
}

@misc{white_sampling_2016,
	title = {Sampling {Generative} {Networks}},
	url = {http://arxiv.org/abs/1609.04468},
	doi = {10.48550/arXiv.1609.04468},
	abstract = {We introduce several techniques for sampling and visualizing the latent spaces of generative models. Replacing linear interpolation with spherical linear interpolation prevents diverging from a model's prior distribution and produces sharper samples. J-Diagrams and MINE grids are introduced as visualizations of manifolds created by analogies and nearest neighbors. We demonstrate two new techniques for deriving attribute vectors: bias-corrected vectors with data replication and synthetic vectors with data augmentation. Binary classification using attribute vectors is presented as a technique supporting quantitative analysis of the latent space. Most techniques are intended to be independent of model type and examples are shown on both Variational Autoencoders and Generative Adversarial Networks.},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {White, Tom},
	month = dec,
	year = {2016},
	note = {arXiv:1609.04468 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\jackb\\Zotero\\storage\\4XYFDN6F\\White - 2016 - Sampling Generative Networks.pdf:application/pdf;Snapshot:C\:\\Users\\jackb\\Zotero\\storage\\SS373DDZ\\1609.html:text/html},
}
