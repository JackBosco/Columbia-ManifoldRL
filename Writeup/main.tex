\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{mathptmx} 

\geometry{top=0.5in, bottom=1in, left=1in, right=1in}

\begin{document}

\title{Reinforcement Learning Project Proposal}
\author{Joseph (Jack) Bosco \\ jab2516 \and Nikolaus Holzer \\ nh2677 \and Akshara Pramod \\ ap4613}
\date{Date : 11th March, 2025}

\maketitle 
\noindent \textbf{Idea}\\
\\

\noindent \textbf{Motivation} \\
For AI models to be safely deployed, especially in environments where human safety or well-being is involved, stakeholders must be able to interpret and trust the decisions made by these models. In RL, particularly with deep learning models like DQN, understanding why an agent chooses a particular action can be challenging due to the complexity of the learned policies. As part of this project, we aim to solve this issue using a custom reward mechanism : KL divergence, using OpenAI Gym's MountainCar-v0 dataset. We wish to achieve a generalization that can be further expanded for more complex AI systems. 
\\

\noindent \textbf{Strategy}\\
\\

\noindent \textbf{Data}\\
\\

\end{document}
